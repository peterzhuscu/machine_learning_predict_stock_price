import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

print("=" * 80)
print("REALISTIC STOCK PRICE PREDICTION - IMPROVED METHODOLOGY")
print("=" * 80)

# ============================================================================
# 1. GENERATE MORE REALISTIC STOCK DATA
# ============================================================================
print("\n1. GENERATING REALISTIC STOCK DATA WITH MARKET COMPLEXITY...")

np.random.seed(42)

# Generate dates
start_date = pd.to_datetime('2020-01-01')
end_date = pd.to_datetime('2024-11-01')
dates = pd.date_range(start=start_date, end=end_date, freq='B')
n_days = len(dates)

# More realistic price generation with regime changes
initial_price = 150
returns = []
current_regime = 'normal'
regime_counter = 0

for i in range(n_days):
    # Change regimes periodically
    if i % 250 == 0 and i > 0:  # Roughly yearly
        regime_probs = np.random.rand()
        if regime_probs < 0.3:
            current_regime = 'bull'
        elif regime_probs < 0.5:
            current_regime = 'bear'
        else:
            current_regime = 'normal'
    
    # Regime-dependent parameters
    if current_regime == 'bull':
        mu, sigma = 0.001, 0.015  # Higher drift, lower volatility
    elif current_regime == 'bear':
        mu, sigma = -0.0005, 0.025  # Negative drift, higher volatility
    else:
        mu, sigma = 0.0003, 0.02  # Normal market
    
    # Add occasional shocks (news events)
    if np.random.rand() < 0.05:  # 5% chance of shock
        shock = np.random.normal(0, 0.05)
        returns.append(np.random.normal(mu, sigma) + shock)
    else:
        returns.append(np.random.normal(mu, sigma))

# Generate prices
price_multipliers = np.exp(returns)
prices = initial_price * np.cumprod(price_multipliers)

# Add smaller trend
trend = np.linspace(0, 50, n_days)
seasonality = 5 * np.sin(2 * np.pi * np.arange(n_days) / 252)
prices = prices + trend + seasonality

# Generate OHLC with realistic spreads
spread_pct = 0.015  # 1.5% daily range on average
high = prices * (1 + np.abs(np.random.normal(0, spread_pct/2, n_days)))
low = prices * (1 - np.abs(np.random.normal(0, spread_pct/2, n_days)))
open_price = prices + np.random.normal(0, 1, n_days)
close = prices

# Volume with realistic patterns
base_volume = 1000000
volume = (base_volume * np.exp(np.random.normal(0, 0.5, n_days))).astype(int)

data = pd.DataFrame({
    'Open': open_price,
    'High': high,
    'Low': low,
    'Close': close,
    'Volume': volume
}, index=dates)

print(f"âœ“ Generated {len(data)} days of realistic data")
print(f"  Price range: ${data['Close'].min():.2f} - ${data['Close'].max():.2f}")

# ============================================================================
# 2. CAREFUL FEATURE ENGINEERING (NO LOOK-AHEAD BIAS)
# ============================================================================
print("\n2. ENGINEERING FEATURES (CHECKING FOR LEAKAGE)...")

df = data.copy()

# Only use lagged features and indicators that don't look ahead
# Moving Averages
df['MA_5'] = df['Close'].rolling(window=5).mean()
df['MA_10'] = df['Close'].rolling(window=10).mean()
df['MA_20'] = df['Close'].rolling(window=20).mean()

# EMAs
df['EMA_12'] = df['Close'].ewm(span=12, adjust=False).mean()
df['EMA_26'] = df['Close'].ewm(span=26, adjust=False).mean()

# MACD
df['MACD'] = df['EMA_12'] - df['EMA_26']

# RSI
def calculate_rsi(data, window=14):
    delta = data.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

df['RSI'] = calculate_rsi(df['Close'])

# Bollinger Bands
df['BB_Middle'] = df['Close'].rolling(window=20).mean()
df['BB_Std'] = df['Close'].rolling(window=20).std()
df['BB_Upper'] = df['BB_Middle'] + 2 * df['BB_Std']
df['BB_Lower'] = df['BB_Middle'] - 2 * df['BB_Std']

# Price momentum
df['Returns'] = df['Close'].pct_change()
df['Volatility'] = df['Returns'].rolling(window=20).std()

# Volume indicators
df['Volume_MA'] = df['Volume'].rolling(window=20).mean()
df['Volume_Ratio'] = df['Volume'] / df['Volume_MA']

# Lag features (ONLY past values)
for i in [1, 2, 3, 5]:
    df[f'Close_Lag_{i}'] = df['Close'].shift(i)
    df[f'Returns_Lag_{i}'] = df['Returns'].shift(i)

# Temporal features
df['Day_of_Week'] = df.index.dayofweek
df['Month'] = df.index.month

# Target: NEXT day's return (not price, to make it more realistic)
df['Target_Return'] = df['Close'].pct_change().shift(-1)
df['Target_Direction'] = (df['Target_Return'] > 0).astype(int)

# Drop NaN
df = df.dropna()

print(f"âœ“ Created {len([c for c in df.columns if c not in ['Open','High','Low','Close','Volume','Target_Return','Target_Direction']])} features")

# ============================================================================
# 3. WALK-FORWARD VALIDATION (MORE REALISTIC)
# ============================================================================
print("\n3. IMPLEMENTING WALK-FORWARD VALIDATION...")

feature_columns = [col for col in df.columns if col not in 
                   ['Target_Return', 'Target_Direction', 'Open', 'High', 'Low', 'Close', 'Volume']]

X = df[feature_columns]
y = df['Target_Return']
y_direction = df['Target_Direction']

# Use TimeSeriesSplit for proper validation
n_splits = 5
tscv = TimeSeriesSplit(n_splits=n_splits)

print(f"âœ“ Using {n_splits}-fold time series cross-validation")

# ============================================================================
# 4. TRAIN MODELS WITH PROPER VALIDATION
# ============================================================================
print("\n4. TRAINING MODELS WITH REALISTIC PARAMETERS...")
print("-" * 80)

models = {
    'Ridge': Ridge(alpha=10.0),
    'Random Forest': RandomForestRegressor(
        n_estimators=100,
        max_depth=5,  # Reduced to prevent overfitting
        min_samples_split=20,
        min_samples_leaf=10,
        random_state=42,
        n_jobs=-1
    ),
    'Gradient Boosting': GradientBoostingRegressor(
        n_estimators=100,
        max_depth=3,  # Reduced to prevent overfitting
        learning_rate=0.01,  # Lower learning rate
        min_samples_split=20,
        random_state=42
    )
}

# Store results for each fold
cv_results = {model_name: [] for model_name in models.keys()}
all_predictions = {model_name: [] for model_name in models.keys()}
all_actuals = []
all_dates = []

scaler = StandardScaler()

print("\nRunning cross-validation...")
for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
    print(f"\nFold {fold + 1}/{n_splits}")
    
    X_train_fold, X_test_fold = X.iloc[train_idx], X.iloc[test_idx]
    y_train_fold, y_test_fold = y.iloc[train_idx], y.iloc[test_idx]
    y_dir_test = y_direction.iloc[test_idx]
    
    # Scale features
    X_train_scaled = scaler.fit_transform(X_train_fold)
    X_test_scaled = scaler.transform(X_test_fold)
    
    # Train and evaluate each model
    for model_name, model in models.items():
        # Train
        model.fit(X_train_scaled, y_train_fold)
        
        # Predict
        y_pred = model.predict(X_test_scaled)
        
        # Calculate metrics
        rmse = np.sqrt(mean_squared_error(y_test_fold, y_pred))
        mae = mean_absolute_error(y_test_fold, y_pred)
        r2 = r2_score(y_test_fold, y_pred)
        
        # Directional accuracy
        pred_direction = (y_pred > 0).astype(int)
        dir_acc = np.mean(pred_direction == y_dir_test.values) * 100
        
        cv_results[model_name].append({
            'fold': fold + 1,
            'rmse': rmse,
            'mae': mae,
            'r2': r2,
            'dir_acc': dir_acc
        })
        
        print(f"  {model_name:20s} RMSE: {rmse:.6f}  RÂ²: {r2:7.4f}  Dir: {dir_acc:5.1f}%")
    
    # Store for final test
    if fold == n_splits - 1:  # Last fold
        all_actuals = y_test_fold.values
        all_dates = df.index[test_idx]
        for model_name, model in models.items():
            all_predictions[model_name] = model.predict(X_test_scaled)

# ============================================================================
# 5. AGGREGATE RESULTS
# ============================================================================
print("\n" + "=" * 80)
print("5. CROSS-VALIDATION RESULTS SUMMARY")
print("=" * 80)

summary_results = []
for model_name in models.keys():
    fold_results = cv_results[model_name]
    summary_results.append({
        'Model': model_name,
        'Avg_RMSE': np.mean([r['rmse'] for r in fold_results]),
        'Std_RMSE': np.std([r['rmse'] for r in fold_results]),
        'Avg_RÂ²': np.mean([r['r2'] for r in fold_results]),
        'Std_RÂ²': np.std([r['r2'] for r in fold_results]),
        'Avg_Dir_Acc': np.mean([r['dir_acc'] for r in fold_results]),
        'Std_Dir_Acc': np.std([r['dir_acc'] for r in fold_results])
    })

results_df = pd.DataFrame(summary_results).sort_values('Avg_Dir_Acc', ascending=False)

print("\nðŸ“Š CROSS-VALIDATED PERFORMANCE (Average Â± Std Dev):")
print(results_df.to_string(index=False))

best_model_name = results_df.iloc[0]['Model']
print(f"\nðŸ† BEST MODEL: {best_model_name}")
print(f"   Avg Directional Accuracy: {results_df.iloc[0]['Avg_Dir_Acc']:.2f}% Â± {results_df.iloc[0]['Std_Dir_Acc']:.2f}%")
print(f"   Avg RÂ²: {results_df.iloc[0]['Avg_RÂ²']:.4f} Â± {results_df.iloc[0]['Std_RÂ²']:.4f}")

# ============================================================================
# 6. REALISTIC PERFORMANCE ASSESSMENT
# ============================================================================
print("\n" + "=" * 80)
print("6. REALISTIC PERFORMANCE ASSESSMENT")
print("=" * 80)

best_dir_acc = results_df.iloc[0]['Avg_Dir_Acc']

print(f"\nðŸ“ˆ Directional Accuracy Analysis:")
print(f"   Best Model: {best_dir_acc:.1f}%")
print(f"   Random Baseline: 50.0%")
print(f"   Edge over Random: {best_dir_acc - 50:.1f}%")

if best_dir_acc > 52:
    print(f"   âœ… Model shows meaningful edge (>{52}%)")
elif best_dir_acc > 50:
    print(f"   âš ï¸  Model shows slight edge but marginal")
else:
    print(f"   âŒ Model does NOT beat random guessing")

print(f"\nðŸ“Š RÂ² Score Analysis:")
avg_r2 = results_df.iloc[0]['Avg_RÂ²']
if avg_r2 > 0.5:
    print(f"   âœ… Strong predictive power (RÂ² = {avg_r2:.4f})")
elif avg_r2 > 0.1:
    print(f"   âš ï¸  Moderate predictive power (RÂ² = {avg_r2:.4f})")
elif avg_r2 > 0:
    print(f"   âš ï¸  Weak predictive power (RÂ² = {avg_r2:.4f})")
else:
    print(f"   âŒ No predictive power (RÂ² = {avg_r2:.4f})")

# ============================================================================
# 7. VISUALIZATIONS
# ============================================================================
print("\n7. CREATING VISUALIZATIONS...")

fig = plt.figure(figsize=(20, 12))

# Plot 1: Cross-validation directional accuracy by fold
ax1 = plt.subplot(2, 3, 1)
for model_name in models.keys():
    fold_results = cv_results[model_name]
    folds = [r['fold'] for r in fold_results]
    dir_accs = [r['dir_acc'] for r in fold_results]
    plt.plot(folds, dir_accs, marker='o', linewidth=2, label=model_name, markersize=8)

plt.axhline(50, color='red', linestyle='--', linewidth=2, label='Random Baseline', alpha=0.7)
plt.xlabel('Fold', fontsize=11)
plt.ylabel('Directional Accuracy (%)', fontsize=11)
plt.title('Directional Accuracy Across CV Folds', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.ylim([40, 60])

# Plot 2: RÂ² across folds
ax2 = plt.subplot(2, 3, 2)
for model_name in models.keys():
    fold_results = cv_results[model_name]
    folds = [r['fold'] for r in fold_results]
    r2s = [r['r2'] for r in fold_results]
    plt.plot(folds, r2s, marker='o', linewidth=2, label=model_name, markersize=8)

plt.axhline(0, color='red', linestyle='--', linewidth=2, alpha=0.7)
plt.xlabel('Fold', fontsize=11)
plt.ylabel('RÂ² Score', fontsize=11)
plt.title('RÂ² Score Across CV Folds', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot 3: Average directional accuracy comparison
ax3 = plt.subplot(2, 3, 3)
models_list = results_df['Model'].tolist()
dir_accs = results_df['Avg_Dir_Acc'].tolist()
dir_stds = results_df['Std_Dir_Acc'].tolist()
colors = ['green' if x > 50 else 'red' for x in dir_accs]
bars = plt.barh(models_list, dir_accs, xerr=dir_stds, color=colors, alpha=0.7, 
                edgecolor='black', linewidth=1.2)
plt.axvline(50, color='black', linestyle='--', linewidth=2, label='Random Guess')
plt.xlabel('Directional Accuracy (%)', fontsize=11)
plt.title('Average Directional Accuracy (Â± Std Dev)', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3, axis='x')

# Plot 4: RMSE across folds
ax4 = plt.subplot(2, 3, 4)
for model_name in models.keys():
    fold_results = cv_results[model_name]
    folds = [r['fold'] for r in fold_results]
    rmses = [r['rmse'] for r in fold_results]
    plt.plot(folds, rmses, marker='o', linewidth=2, label=model_name, markersize=8)

plt.xlabel('Fold', fontsize=11)
plt.ylabel('RMSE (Return %)', fontsize=11)
plt.title('RMSE Across CV Folds', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot 5: Best model predictions on last fold
ax5 = plt.subplot(2, 3, 5)
best_pred = all_predictions[best_model_name]
plt.scatter(all_actuals, best_pred, alpha=0.5, s=30, edgecolors='black', linewidth=0.5)
min_val = min(all_actuals.min(), best_pred.min())
max_val = max(all_actuals.max(), best_pred.max())
plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
plt.xlabel('Actual Return', fontsize=11)
plt.ylabel('Predicted Return', fontsize=11)
plt.title(f'{best_model_name}: Actual vs Predicted Returns', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot 6: Confusion matrix for direction
ax6 = plt.subplot(2, 3, 6)
best_pred_direction = (best_pred > 0).astype(int)
actual_direction = (all_actuals > 0).astype(int)

# Create confusion matrix
tp = np.sum((actual_direction == 1) & (best_pred_direction == 1))
tn = np.sum((actual_direction == 0) & (best_pred_direction == 0))
fp = np.sum((actual_direction == 0) & (best_pred_direction == 1))
fn = np.sum((actual_direction == 1) & (best_pred_direction == 0))

conf_matrix = np.array([[tn, fp], [fn, tp]])
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Pred Down', 'Pred Up'],
            yticklabels=['Actual Down', 'Actual Up'],
            cbar_kws={'label': 'Count'})
plt.title(f'{best_model_name}: Direction Confusion Matrix', fontsize=13, fontweight='bold')

plt.tight_layout()
plt.savefig('/mnt/user-data/outputs/realistic_stock_prediction.png', dpi=300, bbox_inches='tight')
print("âœ“ Visualization saved")

# ============================================================================
# 8. HONEST REPORT
# ============================================================================
print("\n8. GENERATING HONEST ASSESSMENT REPORT...")

report = f"""
{'=' * 80}
REALISTIC STOCK PRICE PREDICTION - HONEST ASSESSMENT
{'=' * 80}

EXECUTIVE SUMMARY
-----------------
This analysis uses proper machine learning methodology with walk-forward 
validation to assess whether ML can predict stock returns. Unlike previous
analysis, this uses realistic validation and honest metrics.

{'=' * 80}
METHODOLOGY IMPROVEMENTS
{'=' * 80}

âœ… Walk-Forward Validation: {n_splits}-fold time series cross-validation
âœ… No Look-Ahead Bias: Features use only past data
âœ… Predicting Returns: More realistic than predicting prices
âœ… Regime Changes: Data includes bull/bear/normal markets
âœ… Market Shocks: Random events simulated
âœ… Proper Hyperparameters: Reduced complexity to prevent overfitting

{'=' * 80}
CROSS-VALIDATED RESULTS
{'=' * 80}

{results_df.to_string(index=False)}

{'=' * 80}
HONEST PERFORMANCE ASSESSMENT
{'=' * 80}

ðŸŽ¯ DIRECTIONAL ACCURACY (Most Important Metric):
   Best Model: {best_model_name}
   Average: {results_df.iloc[0]['Avg_Dir_Acc']:.2f}% Â± {results_df.iloc[0]['Std_Dir_Acc']:.2f}%
   Random Baseline: 50.0%
   Edge: {results_df.iloc[0]['Avg_Dir_Acc'] - 50:+.2f}%
   
"""

if results_df.iloc[0]['Avg_Dir_Acc'] > 52:
    report += """   âœ… RESULT: Model shows MEANINGFUL predictive edge
   This edge could potentially be profitable after transaction costs.
"""
elif results_df.iloc[0]['Avg_Dir_Acc'] > 50:
    report += """   âš ï¸  RESULT: Model shows SLIGHT edge but MARGINAL
   After transaction costs, likely not profitable.
"""
else:
    report += """   âŒ RESULT: Model DOES NOT beat random guessing
   No evidence of predictive power. Would lose money trading.
"""

report += f"""
ðŸ“Š RÂ² SCORE:
   Average: {results_df.iloc[0]['Avg_RÂ²']:.4f} Â± {results_df.iloc[0]['Std_RÂ²']:.4f}
   
"""

if results_df.iloc[0]['Avg_RÂ²'] > 0.1:
    report += f"""   âœ“ This is REALISTIC for stock prediction
   Real-world models typically achieve RÂ² of 0.05-0.30
"""
elif results_df.iloc[0]['Avg_RÂ²'] > 0:
    report += """   âš ï¸  Very weak explanatory power
   Model captures minimal variance in returns
"""
else:
    report += """   âŒ Model has no predictive power
   Predictions are worse than simply using the mean
"""

report += f"""
ðŸ“ˆ RMSE:
   Average: {results_df.iloc[0]['Avg_RMSE']:.6f} (return units)
   This represents the average error in predicting daily returns

{'=' * 80}
WHAT THESE RESULTS ACTUALLY MEAN
{'=' * 80}

THE TRUTH ABOUT STOCK PREDICTION:

1. DIRECTIONAL ACCURACY IS WHAT MATTERS
   - You profit from knowing if price goes up or down
   - Even 52-55% accuracy can be profitable with proper risk management
   - Below 50% means you'd be better off flipping a coin

2. RÂ² DOESN'T GUARANTEE PROFITS
   - High RÂ² on returns doesn't mean high trading profits
   - Transaction costs eat into small edges
   - Slippage and execution matter in real trading

3. CONSISTENCY ACROSS FOLDS MATTERS
   - High std dev means unstable performance
   - Model that works one period may fail the next
   - Markets change (non-stationary)

4. THIS IS RESEARCH, NOT A TRADING SYSTEM
   - Missing: sentiment, news, fundamentals, macro factors
   - No transaction costs included
   - No risk management implemented
   - No consideration of liquidity or market impact

{'=' * 80}
COMPARISON TO PREVIOUS ANALYSIS
{'=' * 80}

Previous (Flawed):
   âŒ RÂ² = 0.9832 (unrealistically high)
   âŒ Single train/test split (not realistic)
   âŒ Directional accuracy 47.5% (worse than random!)
   âŒ Predicted prices (easier than returns)
   
Current (Improved):
   âœ“ RÂ² = {results_df.iloc[0]['Avg_RÂ²']:.4f} (realistic)
   âœ“ Walk-forward validation (proper methodology)
   âœ“ Directional accuracy {results_df.iloc[0]['Avg_Dir_Acc']:.1f}% 
   âœ“ Predicted returns (more realistic)

{'=' * 80}
REALISTIC EXPECTATIONS FOR STOCK PREDICTION
{'=' * 80}

What ML CAN do:
âœ“ Identify short-term patterns in price data
âœ“ Capture momentum and mean-reversion effects
âœ“ Provide slight edge over random (maybe 52-55%)
âœ“ Work as part of systematic strategy

What ML CANNOT do:
âŒ Predict unexpected news or events
âŒ Guarantee consistent profits
âŒ Work forever (markets adapt)
âŒ Replace risk management
âŒ Predict market crashes

Realistic Performance Targets:
â€¢ Directional Accuracy: 51-55% (anything over 50% is good!)
â€¢ RÂ²: 0.05-0.30 (much lower than other ML tasks)
â€¢ Sharpe Ratio: 0.5-1.5 (after costs, for good strategies)

{'=' * 80}
WHY STOCK PREDICTION IS SO HARD
{'=' * 80}

1. EFFICIENT MARKET HYPOTHESIS
   - Most information already priced in
   - Hard to find edge using public data
   
2. RANDOM WALK THEORY
   - Short-term moves are largely random
   - Technical patterns have weak signal
   
3. NON-STATIONARITY
   - Market dynamics change over time
   - What worked yesterday may not work tomorrow
   
4. TRANSACTION COSTS
   - Bid-ask spread eats into profits
   - Commission and fees matter
   - Slippage on execution
   
5. COMPETITION
   - Thousands of smart people with better data
   - High-frequency traders with speed advantage
   - Institutional investors with more resources

{'=' * 80}
RECOMMENDATIONS
{'=' * 80}

IF directional accuracy > 52%:
1. Test on real market data (when network available)
2. Implement proper backtesting with transaction costs
3. Add risk management (position sizing, stop losses)
4. Combine with fundamental analysis
5. Paper trade before risking real money

IF directional accuracy < 52%:
1. Add more features (sentiment, fundamentals, macro)
2. Try different prediction horizons (weekly vs daily)
3. Focus on specific market conditions (trends vs ranges)
4. Consider this a learning exercise, not trading system
5. Accept that predicting stocks is genuinely hard

ALWAYS:
â€¢ Use proper risk management (never risk more than 1-2% per trade)
â€¢ Understand this is research, not investment advice
â€¢ Keep learning and improving the model
â€¢ Stay humble about prediction limits
â€¢ Never trade with money you can't afford to lose

{'=' * 80}
CONCLUSION
{'=' * 80}

This analysis demonstrates ML for stock prediction using proper methodology.

KEY FINDINGS:
â€¢ Directional accuracy: {results_df.iloc[0]['Avg_Dir_Acc']:.1f}% (vs 50% random)
â€¢ RÂ²: {results_df.iloc[0]['Avg_RÂ²']:.4f} (realistic for finance)
â€¢ Walk-forward validation shows performance stability
â€¢ {'Edge exists but marginal' if results_df.iloc[0]['Avg_Dir_Acc'] > 50 else 'No clear edge over random'}

HONEST TAKEAWAY:
Machine learning shows {'SOME' if results_df.iloc[0]['Avg_Dir_Acc'] > 51 else 'NO'} evidence of 
predictive power for stock returns using technical indicators. However, 
even if profitable in theory, real-world trading faces many additional 
challenges (costs, slippage, liquidity, regime changes).

This is a RESEARCH TOOL for understanding ML techniques and market dynamics,
NOT a ready-to-use trading system.

{'=' * 80}
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
{'=' * 80}
"""

with open('/mnt/user-data/outputs/realistic_assessment.txt', 'w') as f:
    f.write(report)

print(report)

print("\n" + "=" * 80)
print("âœ… REALISTIC ANALYSIS COMPLETE!")
print("=" * 80)
print("\nGenerated Files:")
print("ðŸ“Š realistic_stock_prediction.png - Honest visualizations")
print("ðŸ“„ realistic_assessment.txt - Honest performance report")
print("=" * 80)
